{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fa2f6d",
   "metadata": {},
   "source": [
    "# Template to reproduce EMS proceedings figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a5819",
   "metadata": {},
   "source": [
    "let's start with vertical cross sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a779306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_collection import *\n",
    "\n",
    "import xarray as xr\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "\n",
    "#####         LOAD DATA        #####\n",
    "#####       SST and PBLH       #####\n",
    "##### and apply daily average  #####\n",
    "\n",
    "\n",
    "#####  in the paper we select the offshore domain #####\n",
    "#####    by multiplying each field by this mask   #####\n",
    "'''\n",
    "sea_mask = np.load('/path/to/sea_mask.npy')\n",
    "sea_mask_nan = np.ones_like(sea_mask)\n",
    "sea_mask_nan[sea_mask==0] = np.nan\n",
    "field = field*sea_mask_nan\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49faf149",
   "metadata": {},
   "source": [
    " the control variable in this work is SST: \n",
    " we compute its mesoscale anomalies with the nan_gaussian_filter() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16653b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 100  ## corresponds to 200 km , since model gridspace is 2 km\n",
    "\n",
    "sst_anomalies = np.zeros(sst_day.shape)\n",
    "for t in range(0,sst_day.shape[0]):\n",
    "    if t % 10 == 0:\n",
    "        print(t)\n",
    "    \n",
    "    sst_ave = nan_gaussian_filter(sst_day[t], sigma)                             \n",
    "    sst_anomalies[t] = sst_day[t] - sst_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878c3f5",
   "metadata": {},
   "source": [
    " compute level-by-level mesoscale anomalies \n",
    " of a 3D field, such as the Brunt-Vaisala frequency squared (N2.nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd62dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThreeD_field_anomalies = np.zeros(ThreeD_field_day.shape)\n",
    "\n",
    "for t in range(0, ThreeD_field_day.shape[0]):        # loop in time\n",
    "    if t % 10 == 0:\n",
    "        print(t)\n",
    "        \n",
    "    for h in range(0,ThreeD_field_day.shape[1]):     # loop in vertical levels\n",
    "        ThreeD_field_ave = gm.nan_gaussian_filter(ThreeD_field_day[t,h],s)                               \n",
    "        ThreeD_field_anomalies[t,h] = ThreeD_field_day[t,h] - ThreeD_field_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute percentile bin distributions\n",
    "perc_step = 5\n",
    "nbins = int(100/perc_step)\n",
    "\n",
    "nskip = 15       ## == 30 km\n",
    "nt = 1\n",
    "nskiptop = 75    ## == 150 km\n",
    "nttop = 1\n",
    "\n",
    "pdist_dsst, pdist_dThreeD_field, pstd_dThreeD_field, pstderr_dThreeD_field, pnpoints_dThreeD_field, ppvalue_dThreeD_field_sub = \\\n",
    "dist_3d_subsample(sst_anomalies, ThreeD_field_anomalies, perc_step, nbins, popmean=0, nt, nttop, nskip, nskiptop, top=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0eb26c",
   "metadata": {},
   "source": [
    "# The resulting distributions may be found in the NetCDF files we have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load monthly mean air column (units: hPa)\n",
    "mean_vert_pres = np.load('/path/to/mean_vert_pres.npy')\n",
    "\n",
    "'''\n",
    "## compute percentile distributions \n",
    "## of MABLH in pressure coordinates\n",
    "pdist_dsst, pdist_MABLH_dsst, pstd_MABLH_dsst, pstderr_MABLH_dsst, pnpoints_MABLH_dsst, ppvalue_MABLH_dsst = \\\n",
    "distrib_2d(sst_anomalies, MABLH_pres, perc_step, nbins, popmean=0)\n",
    "'''\n",
    "\n",
    "## load MABLH percentile distributions\n",
    "ds_MABLH = xr.open_dataset('path/to/pdistrs_MABLH_day.nc')\n",
    "pdist_dsst = ds_MABLH['pdist_dsst'].values\n",
    "pdist_MABLH_dsst = ds_MABLH['pdist_MABLH_day'].values\n",
    "pstd_MABLH_dsst = ds_MABLH['pstd_MABLH_day'].values\n",
    "pstderr_MABLH_dsst = ds_MABLH['pstderr_MABLH_day'].values\n",
    "pnpoints_MABLH_dsst = ds_MABLH['pnpoints_MABLH_day'].values\n",
    "\n",
    "\n",
    "## load 3D distributions of \n",
    "## atmospheric mesoscale anomalies \n",
    "filename = 'pdistrs_dN2_day.nc'                  # 'pdistrs_dT_day.nc' , 'pdistrs_dQVAPOR_day.nc'\n",
    "ds_3D            = xr.open_dataset(filename)\n",
    "pdist_dsst       = ds_3D['pdist_dsst'].values\n",
    "pdist_3D_dsst    = ds_3D['pdist_dBV_freq_day'].values\n",
    "pstd_3D_dsst     = ds_3D['pstd_dBV_freq_day'].values\n",
    "pstderr_3D_dsst  = ds_3D['pstderr_dBV_freq_day'].values\n",
    "pnpoints_3D_dsst = ds_3D['pnpoints_dBV_freq_day'].values\n",
    "ppvalue_3D_dsst  = ds_3D['ppvalue_dBV_freq_day_sub'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe46d5",
   "metadata": {},
   "source": [
    "## Plot vertical cross sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12296791",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this isolates the view to \n",
    "## the lower troposphere only\n",
    "p_level_plot = int(15)\n",
    "\n",
    "\n",
    "x = pdist_dsst   \n",
    "var = pdist_3D[0:p_level_plot,:] \n",
    "minval = -1.\n",
    "maxval = -minval\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))  \n",
    "\n",
    "\n",
    "ax1.invert_yaxis()\n",
    "p1 = ax1.pcolormesh(x, mean_vert_pres[0:p_level_plot], var, vmin=minval, vmax=maxval, cmap='seismic')\n",
    "ax1.set_xlabel('SST anoms [K]', fontsize=14)\n",
    "ax1.set_ylabel('pressure [hPa]', fontsize=14)\n",
    "ax1.set_title(f\"SST' vs <specify field>' \", fontsize=14)\n",
    "ax1.tick_params(axis='x', labelsize=14) \n",
    "ax1.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "## customize the colorbar\n",
    "cbar = plt.colorbar(p1,ax=ax1, location='right', shrink=0.8, extend='both', pad=0.05)  \n",
    "cbar.set_label(r'anomalies values', fontsize=15)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "## PBLH line plotting\n",
    "x_pbl = x                         \n",
    "var_pbl = pdist_MABLH_dsst    \n",
    "ax1.plot(x_pbl, var_pbl, 'k', linewidth=2.5, label='MABLH')\n",
    "ax1.legend(fontsize=14, loc='upper left')\n",
    "\n",
    "\n",
    "## check which points are statistically significant\n",
    "pval = ppvalue_3D_dsst[0:p_level_plot] \n",
    "GPbin = mean_vert_pres[1:p_level_plot] - np.diff(mean_vert_pres[0:p_level_plot])*0.5\n",
    "signif_Lcorr_H = np.zeros((len(mean_vert_pres[0:p_level_plot])-1,len(x)-1))\n",
    "dsstbin = np.zeros((len(mean_vert_pres[0:p_level_plot])-1,len(x)-1))\n",
    "for h in range(0,len(mean_vert_pres[0:p_level_plot])-1):\n",
    "    cond1 = pval[h,:-1] > 0.05\n",
    "    cond2 = np.abs(var[h,:-1]) < 0.1*np.nanmean(np.abs(var))\n",
    "    cond = (cond1) | (cond2)\n",
    "    signif_Lcorr_H[h,:] = np.zeros(len(x)-1) + np.nan\n",
    "    signif_Lcorr_H[h, cond] = GPbin[h]\n",
    "    dsstbin[h,:] = x[1:] - np.diff(x)*0.5\n",
    "    \n",
    "ax1.scatter(dsstbin,signif_Lcorr_H,  s=0.7, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6229f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3284385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837b62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "950d6bee",
   "metadata": {},
   "source": [
    "# Template for figure 2 (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e03954",
   "metadata": {},
   "source": [
    "### additionally to SST, the variables LH (latent heat flux) and HFX (sensible heat flux) should also be loaded with the proper geographical mask, as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  1)   LOAD SST and LH    #####\n",
    "#####  2)   compute their mesoscale anomalies    #####\n",
    "\n",
    "\n",
    "x = sst_anomalies \n",
    "y = LH_anomalies\n",
    "\n",
    "fit, corcoe, p_value, sigmas = slopes_r_p_mix(x, y, nt, nskip)\n",
    "title= \" Density Scatter SST' - LHF'\"\n",
    "xlabel='SST anomalies [K]'\n",
    "ylabel='LHF anomalies [$W\\,m^{-2}$]'\n",
    "    \n",
    "pos = [0.05, 0.9]\n",
    "fig = density_hexbin(x, y, plot_fit=True, fit=fit, corcoe=corcoe, grdsz=100,\\\n",
    "                     title=title,  \\\n",
    "                     xlabel=xlabel,\\\n",
    "                     ylabel=ylabel,\\\n",
    "                     colormap='inferno', pos=pos, slope_units=\" $W\\,m^{-2}\\,K^{-1}$\")\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=14) \n",
    "plt.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "    \n",
    "if p_value < 0.05:\n",
    "    plt.annotate('slope p < 5%', xy=(pos[0], pos[1]-0.1), \\\n",
    "                         xycoords='axes fraction', fontsize=12, color='k')\n",
    "else:\n",
    "    plt.annotate('slope p > 5%', xy=(pos[0], pos[1]-0.1), \\\n",
    "                         xycoords='axes fraction', fontsize=12, color='k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
